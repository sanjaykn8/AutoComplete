{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bcc82d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nisan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nisan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c75938",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# CONFIG\n",
    "INPUT_CSV = \"Shakespeare.csv\"\n",
    "TEXT_COL = \"PlayerLine\"   # change to the column that contains the text\n",
    "KEEP_SENTENCE_TOKENIZATION = False\n",
    "USE_LEMMATIZATION = False    # set False to use stemming instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "253007ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_script_and_text(s, allowed_script='Latin'):\n",
    "    # quick filter: remove rows with no letters or only weird unicode blocks\n",
    "    if not isinstance(s, str) or s.strip() == \"\":\n",
    "        return False\n",
    "    # crude: ensure presence of at least one Latin letter\n",
    "    return bool(re.search(r'\\p{Latin}', s))\n",
    "\n",
    "def normalize_whitespace(s):\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def remove_special_numbers_punct(s, keep_apostrophe=False):\n",
    "    if keep_apostrophe:\n",
    "        return re.sub(r\"[^0-9A-Za-z'\\s]\", ' ', s)\n",
    "    return re.sub(r'[^0-9A-Za-z\\s]', ' ', s)\n",
    "\n",
    "def remove_numbers(s):\n",
    "    return re.sub(r'\\d+', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af7ef868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_raw(s):\n",
    "    # 1. validate\n",
    "    if not validate_script_and_text(s):\n",
    "        return None\n",
    "    # 2. lowercase\n",
    "    s = s.lower()\n",
    "    # 3. remove special chars & punctuation (choose whether to remove numbers)\n",
    "    s = remove_special_numbers_punct(s, keep_apostrophe=False)\n",
    "    # 4. optionally remove digits\n",
    "    s = remove_numbers(s)\n",
    "    # 5. normalize whitespace\n",
    "    s = normalize_whitespace(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6181c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(s):\n",
    "    return word_tokenize(s)\n",
    "\n",
    "def sentence_tokenize(s):\n",
    "    return sent_tokenize(s)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "def lemmatize_with_spacy(tokens):\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [tok.lemma_ for tok in doc]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [STEMMER.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "030c636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text_by_max_chars(s, max_chars=200):\n",
    "    # naive segmentation preserving sentence boundaries when possible\n",
    "    sents = sentence_tokenize(s)\n",
    "    chunks = []\n",
    "    cur = \"\"\n",
    "    for sent in sents:\n",
    "        if len(cur) + 1 + len(sent) <= max_chars:\n",
    "            cur = (cur + \" \" + sent).strip()\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            if len(sent) > max_chars:\n",
    "                # hard-cut long sentence\n",
    "                for i in range(0, len(sent), max_chars):\n",
    "                    chunks.append(sent[i:i+max_chars].strip())\n",
    "                cur = \"\"\n",
    "            else:\n",
    "                cur = sent\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "706a189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False)\n",
    "\n",
    "# combine columns if needed; here we'll process TEXT_COL but you can combine Play+PlayerLine etc.\n",
    "df['raw_text'] = df[TEXT_COL].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbcbc9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows processed\n",
      "5000 rows processed\n",
      "10000 rows processed\n",
      "15000 rows processed\n",
      "20000 rows processed\n",
      "25000 rows processed\n",
      "30000 rows processed\n",
      "35000 rows processed\n",
      "40000 rows processed\n",
      "45000 rows processed\n",
      "50000 rows processed\n",
      "55000 rows processed\n",
      "60000 rows processed\n",
      "65000 rows processed\n",
      "70000 rows processed\n",
      "75000 rows processed\n",
      "80000 rows processed\n",
      "85000 rows processed\n",
      "90000 rows processed\n",
      "95000 rows processed\n",
      "100000 rows processed\n",
      "105000 rows processed\n",
      "110000 rows processed\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "\n",
    "for i, row in enumerate(df.itertuples(index=False)):\n",
    "    raw = row.raw_text\n",
    "    s = preprocess_text_raw(raw)\n",
    "    if not s:\n",
    "        processed.append(None)\n",
    "        continue\n",
    "\n",
    "    # optional sentence split\n",
    "    if KEEP_SENTENCE_TOKENIZATION:\n",
    "        sents = sentence_tokenize(s)\n",
    "    else:\n",
    "        sents = [s]\n",
    "\n",
    "    all_tokens = []\n",
    "\n",
    "    for sent in sents:\n",
    "        # word tokenization\n",
    "        tokens = tokenize_word(sent)\n",
    "\n",
    "        # remove stopwords (optional but often harmful for autocomplete)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "\n",
    "        # stemming OR lemmatization (usually OFF for autocomplete)\n",
    "        if USE_LEMMATIZATION:\n",
    "            tokens = lemmatize_with_spacy(tokens)\n",
    "        else:\n",
    "            tokens = stem_tokens(tokens)\n",
    "\n",
    "        # final cleanup\n",
    "        tokens = [t for t in tokens if t.strip()]\n",
    "\n",
    "        if tokens:\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "    # store token list\n",
    "    processed.append(all_tokens if all_tokens else None)\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"{i} rows processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed9e9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_segments'] = processed\n",
    "\n",
    "# explode rows if you want one segment per row (helpful for corpora)\n",
    "df_exploded = df.explode('preprocessed_segments')\n",
    "df_exploded = df_exploded.dropna(subset=['preprocessed_segments'])\n",
    "\n",
    "# deduplicate normalized text\n",
    "df_exploded['preprocessed_segments_norm'] = df_exploded['preprocessed_segments'].str.strip().str.lower()\n",
    "df_exploded = df_exploded.drop_duplicates(subset=['preprocessed_segments_norm'])\n",
    "\n",
    "# final filters (e.g., remove very short lines)\n",
    "df_exploded['length'] = df_exploded['preprocessed_segments_norm'].str.len()\n",
    "df_final = df_exploded[(df_exploded['length'] > 2)]  # tune threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3bba2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to processed_output\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"processed_output\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# full df to csv\n",
    "df_final.to_csv(OUT_DIR / \"shakespeare_preprocessed.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "# plain text corpus (one line per segment)\n",
    "with open(OUT_DIR / \"shakespeare_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_final['preprocessed_segments_norm'].tolist():\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# excel (small datasets only)\n",
    "df_final.to_excel(OUT_DIR / \"shakespeare_preprocessed.xlsx\", index=False)\n",
    "\n",
    "print(\"Done. Saved to\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
