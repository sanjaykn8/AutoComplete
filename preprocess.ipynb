{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcc82d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nisan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nisan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c75938",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "INPUT_CSV = \"Shakespeare.csv\"\n",
    "TEXT_COL = \"PlayerLine\"\n",
    "KEEP_SENTENCE_TOKENIZATION = False\n",
    "USE_LEMMATIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253007ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_script_and_text(s, allowed_script='Latin'):\n",
    "    if not isinstance(s, str) or s.strip() == \"\":\n",
    "        return False\n",
    "    return bool(re.search(r'\\p{Latin}', s))\n",
    "\n",
    "def normalize_whitespace(s):\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def remove_special_numbers_punct(s, keep_apostrophe=False):\n",
    "    if keep_apostrophe:\n",
    "        return re.sub(r\"[^0-9A-Za-z'\\s]\", ' ', s)\n",
    "    return re.sub(r'[^0-9A-Za-z\\s]', ' ', s)\n",
    "\n",
    "def remove_numbers(s):\n",
    "    return re.sub(r'\\d+', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7ef868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_raw(s):\n",
    "    if not validate_script_and_text(s):\n",
    "        return None\n",
    "    s = s.lower()\n",
    "    s = remove_special_numbers_punct(s, keep_apostrophe=False)\n",
    "    s = remove_numbers(s)\n",
    "    s = normalize_whitespace(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6181c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(s):\n",
    "    return word_tokenize(s)\n",
    "\n",
    "def sentence_tokenize(s):\n",
    "    return sent_tokenize(s)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "def lemmatize_with_spacy(tokens):\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [tok.lemma_ for tok in doc]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [STEMMER.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030c636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text_by_max_chars(s, max_chars=200):\n",
    "    sents = sentence_tokenize(s)\n",
    "    chunks = []\n",
    "    cur = \"\"\n",
    "    for sent in sents:\n",
    "        if len(cur) + 1 + len(sent) <= max_chars:\n",
    "            cur = (cur + \" \" + sent).strip()\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            if len(sent) > max_chars:\n",
    "                for i in range(0, len(sent), max_chars):\n",
    "                    chunks.append(sent[i:i+max_chars].strip())\n",
    "                cur = \"\"\n",
    "            else:\n",
    "                cur = sent\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706a189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False)\n",
    "\n",
    "df['raw_text'] = df[TEXT_COL].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcbc9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows processed\n",
      "5000 rows processed\n",
      "10000 rows processed\n",
      "15000 rows processed\n",
      "20000 rows processed\n",
      "25000 rows processed\n",
      "30000 rows processed\n",
      "35000 rows processed\n",
      "40000 rows processed\n",
      "45000 rows processed\n",
      "50000 rows processed\n",
      "55000 rows processed\n",
      "60000 rows processed\n",
      "65000 rows processed\n",
      "70000 rows processed\n",
      "75000 rows processed\n",
      "80000 rows processed\n",
      "85000 rows processed\n",
      "90000 rows processed\n",
      "95000 rows processed\n",
      "100000 rows processed\n",
      "105000 rows processed\n",
      "110000 rows processed\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "\n",
    "for i, row in enumerate(df.itertuples(index=False)):\n",
    "    raw = row.raw_text\n",
    "    s = preprocess_text_raw(raw)\n",
    "    if not s:\n",
    "        processed.append(None)\n",
    "        continue\n",
    "\n",
    "    if KEEP_SENTENCE_TOKENIZATION:\n",
    "        sents = sentence_tokenize(s)\n",
    "    else:\n",
    "        sents = [s]\n",
    "\n",
    "    all_tokens = []\n",
    "\n",
    "    for sent in sents:\n",
    "        tokens = tokenize_word(sent)\n",
    "\n",
    "        tokens = remove_stopwords(tokens)\n",
    "\n",
    "        if USE_LEMMATIZATION:\n",
    "            tokens = lemmatize_with_spacy(tokens)\n",
    "        else:\n",
    "            tokens = stem_tokens(tokens)\n",
    "\n",
    "        tokens = [t for t in tokens if t.strip()]\n",
    "\n",
    "        if tokens:\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "    processed.append(all_tokens if all_tokens else None)\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"{i} rows processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed9e9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_segments'] = processed\n",
    "\n",
    "df_exploded = df.explode('preprocessed_segments')\n",
    "df_exploded = df_exploded.dropna(subset=['preprocessed_segments'])\n",
    "\n",
    "df_exploded['preprocessed_segments_norm'] = df_exploded['preprocessed_segments'].str.strip().str.lower()\n",
    "df_exploded = df_exploded.drop_duplicates(subset=['preprocessed_segments_norm'])\n",
    "\n",
    "df_exploded['length'] = df_exploded['preprocessed_segments_norm'].str.len()\n",
    "df_final = df_exploded[(df_exploded['length'] > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bba2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to processed_output\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"processed_output\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_final.to_csv(OUT_DIR / \"shakespeare_preprocessed_l.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "with open(OUT_DIR / \"shakespeare_corpus_l.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df_final['preprocessed_segments_norm'].tolist():\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "df_final.to_excel(OUT_DIR / \"shakespeare_preprocessed_l.xlsx\", index=False)\n",
    "\n",
    "print(\"Done. Saved to\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
